# 引言

## 为什么选择生成模型？

在我们开始思考（深度）生成模型之前，让我们先考虑一个简单的例子。假设我们训练了一个深度神经网络来对动物的图像（$\mathbf{x} \in \mathbb{Z}^D$）进行分类（$y \in \mathcal{Y}, \mathcal{Y}=\{c a t, \text { dog }, \text { horse }\}$）。此外，假设该神经网络经过充分训练，因此它总是以高概率$p(y \mid \mathbf{x})$分类为正确的类别。然而，(Szegedy et al., 2013) 指出，向图像中添加噪声可能会导致完全错误的分类。图1展示了这种情况的一个例子，其中添加噪声可能会改变预测标签的概率分布，但图像几乎没有发生变化。

![](figures/01_introduction/24101701.png)

**图1**：在几乎完全正确分类的图像中添加噪声的示例，这导致了预测标签的偏移。

这个例子表明，用于参数化条件分布$p(y \mid \mathbf{x})$的神经网络缺乏对图像的语义理解。进一步来说，我们甚至可以假设，仅仅学习判别模型不足以进行正确的决策和创建AI。一个机器学习系统不能仅依赖于学习如何做出决策，而不理解现实世界，也不能表达对周围世界的不确定性。如果少量噪声就能改变它的内部置信度，并将其确定性从一个决策转移到另一个决策上，我们如何信任这样的系统？如果它无法正确表达其对周围环境的新颖性判断，我们如何与之交流？

为了说明不确定性和理解在决策中的重要性，让我们考虑一个分类系统，这次是将对象分为两个类别：橙色和蓝色。假设我们有一些二维数据（图2，左侧）以及一个要分类的新数据点（图2中的黑色交叉）。我们可以使用两种方法做出决策。首先，可以通过模型条件分布$p(y \mid \mathbf{x})$（图2，中间）明确地构建分类器。其次，我们可以考虑联合分布$p(\mathbf{x}, y)$，该分布可以进一步分解为$p(\mathbf{x}, y)=p(y \mid \mathbf{x}) p(\mathbf{x})$（图2，右侧）。

![](figures/01_introduction/24101702.png)

**图2**：数据示例（左）和两种决策方法：（中）判别方法，（右）生成方法。

在使用判别方法训练模型后，即条件分布$p(y \mid \mathbf{x})$，我们获得了一个明确的决策边界。然后，我们可以看到黑色交叉点离橙色区域更远，因此分类器为蓝色标签分配了较高的概率。结果，分类器对这个决策非常确定！

另一方面，如果我们额外拟合了分布$p(\mathbf{x})$，我们会发现黑色交叉点不仅离决策边界较远，而且远离蓝色数据点所在的区域。换句话说，黑色点远离高概率区域。结果，黑色交叉点的概率$p(\mathbf{x}=\text { black cross })$较低，联合分布$p(\mathbf{x}=\text { black cross },y=\text{blue})$也较低，因此，决策是不确定的！

这个简单的例子清楚地表明，如果我们想要构建能够做出可靠决策并与人类交流的AI系统，它们首先必须理解环境。为此，它们不能仅仅学习如何做出决策，还应该能够使用概率的语言来量化它们对周围环境的信念Bishop, 2013; Ghahramani, 2015)。为此，我们主张估计对象的分布$p(\mathbf x)$至关重要。

从生成的角度来看，了解分布$p(\mathbf x)$是非常重要的，因为：

- 它可以用来评估给定的对象是否在过去曾经被观察过；
- 它有助于正确权衡决策；
- 它可以用来评估对环境的不确定性；
- 它可以通过与环境互动（例如，通过请求对低$p(\mathbf x)$的对象进行标注）来进行主动学习；
- 最终，它可以用来生成新的对象。

通常在深度学习文献中，生成模型被视为新数据的生成器。然而，在这里我们尝试传达一个新的观点，认为了解$p(\mathbf x)$有更广泛的应用，这对于构建成功的AI系统可能是至关重要的。最后，我们还希望明确与机器学习中生成模型的联系，制定适当的生成过程对于理解感兴趣的现象至关重要Lasserre et al., 2006)。然而，在许多情况下，关注另一种概率分解，即$p(\mathbf{x}, y)=p(\mathbf{x} \mid y) p(y)$会更容易。我们认为，考虑$p(\mathbf{x}, y)=p(\mathbf{x} \mid y) p(y)$有其明显的优势。



## （深度）生成模型的应用  

随着神经网络的发展和计算能力的提高，深度生成模型已成为AI的主要方向之一。其应用涵盖了机器学习中典型的模式，即文本分析Bowman et al., 2015)、图像分析(Goodfellow et al., 2014)、音频分析(v.d. Oord et al., 2016a)，到主动学习(Sinha et al., 2019)、强化学习(Ha & Schmidhuber, 2018)、图分析(Simonovsky & Komodakis, 2018)和医学影像(Ilse et al., 2020)的问题。图3中，我们直观地展示了深度生成模型的潜在应用。

![](figures/01_introduction/24101703.png)

**图3**：深度生成模型的各种应用。

在某些应用中，确实需要生成对象或修改对象的特征来创建新对象（例如，一个应用程序将年轻人变成老人）。然而，在主动学习中，重要的是请求对不确定的对象（即，低$p(\mathbf{x})$）进行标注。在强化学习中，生成下一个最可能的情况（状态）对于代理执行行动至关重要。对于医学应用，解释决策（例如，以标签的概率和对象的形式）对人类医生来说显然比仅仅辅助诊断标签更具信息性。如果AI系统能够指示其确定性，并量化该对象是否可疑（即，低$p(\mathbf{x})$），那么它可以作为一个独立的专家，提供自己的意见。

这些例子清楚地表明，许多领域都能从（深度）生成模型中受益。显然，AI系统需要具备许多机制。然而，我们主张生成模型能力无疑是最重要的机制之一，正如上述案例所强调的。



## 如何构建（深度）生成模型？  

在这个时候，在强调了（深度）生成模型的重要性和广泛适用性之后，我们应该问自己如何构建（深度）生成模型。换句话说，如何表达我们已经多次提到的$p(\mathbf x)$。

我们可以将（深度）生成模型分为三大类（见图4）：

- 自回归生成模型（ARM）；
- 基于流的模型；
- 潜变量模型；

我们在括号中使用了“深度”这个词，因为到目前为止，我们讨论的大多数内容可以不使用神经网络进行模型。然而，神经网络具有灵活性和强大性，因此广泛用于参数化生成模型。从现在起，我们将完全专注于深度生成模型。

顺便提一下，我们还想提到其他深度生成模型，例如能量模型(Hinton & Ghahramani, 1997)或最近提出的深度扩散模型(Ho et al., 2020)。在这里，我们主要关注在深度生成模型文献中最受欢迎的三类模型。

![](figures/01_introduction/24101704.png)

**图4**：深度生成模型的示意图。



### 自回归模型

第一类深度生成模型利用了自回归模型（ARM）的思想。换句话说，$\mathbf x$的分布以自回归的方式表示：
$$
p(\mathbf{x})=p\left(x_0\right) \prod_{i=1}^D p\left(x_i \mid \mathbf{x}_{<i}\right),
$$
其中$\mathbf x_{<i}$表示所有$i$之前的$\mathbf x$。

建模所有条件分布$p\left(x_i \mid \mathbf{x}_{<i}\right)$在计算上是低效的。然而，我们可以利用因果卷积，如 (v.d. Oord et al., 2016a)对音频的展示，以及 (v.d. Oord et al., 2016b) 对图像的展示。



### 基于流的模型  

变量代换公式通过可逆变换$f$ (Rippel & Adams, 2013)为表达随机变量的密度提供了原则性方法：
$$
p(\mathbf{x})=p(\mathbf{z}=f(\mathbf{x}))\left|\mathbf{J}_{f(\mathbf{x})}\right|
$$
其中$\mathbf{J}_{f(\mathbf{x})}$表示雅可比矩阵。

我们可以使用深度神经网络对$f$进行参数化，然而，它不能是任意的神经网络，因为我们必须能够计算雅可比矩阵。最早的尝试集中在volume-preserving的线性变换上，即$|\mathbf{J}_{f(\mathbf{x})}|=1$(Dinh et al., 2014; Tomczak & Welling, 2016)。进一步的尝试利用了矩阵行列式定理，得到了特定的非线性变换，即平面流(Rezende & Mohamed, 2015)和Sylvester流(van den Berg et al., 2018; Hoogeboom et al., 2020)。另一种方法侧重于为可逆变换构建容易计算雅可比行列式的变换层，如RealNVP (Dinh et al., 2016)。